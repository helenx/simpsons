{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of scripts by clustering text\n",
    "\n",
    "We'll use the Natural Language Toolkit to further explore scripts from the Simpsons. First, we will need to clean up the text and extract keywords, then we can use other natural language processing methods to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import time\n",
    "%matplotlib inline\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to PostgreSQL database of Simpsons scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://hsf001@localhost/simpsonsscripts\n"
     ]
    }
   ],
   "source": [
    "# connect to postgresl\n",
    "dbname = 'simpsonsscripts'\n",
    "username = 'hsf001'\n",
    "\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull scripts from one season\n",
    "\n",
    "We'll focus on one season first and pull the text from Season 8 scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts from season 8\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT ep.number, ep.name, scripts.text\n",
    "FROM episodes ep  \n",
    "  LEFT JOIN scripts ON ep.url = scripts.url\n",
    "WHERE ep.season='8'\n",
    "\"\"\"\n",
    "\n",
    "sql_out = pd.read_sql_query(sql_query,con)\n",
    "sql_out['number']=sql_out['number'].astype(int)\n",
    "sql_out.sort_values(by='number', inplace=True)\n",
    "\n",
    "textList = sql_out['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for one season of the Simpsons\n",
    "\n",
    "Let's try using a common analysis method, term frequency - inverse document frequency (TF-IDF), and look at what words or phrases might be most representative of an episode. We'll do this for the episodes from season 8 following [this document clustering guide](http://brandonrose.org/clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "First, we will need to process our data by formatting the text and removing meaningless words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "[Stop words](https://en.wikipedia.org/wiki/Stop_words) often consist of articles, pronouns, prepositions, etc. that don't convey significant meaning and may vary depending on different situations. We're going to use the [NLTK](http://www.nltk.org/) list of English stop words here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# load NLTK's English stop words as variable stopwords\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) is used to break words down to their roots. For example, 'running' and 'run' have the same root 'run.' We'll use the [Snowball Stemmer](http://snowballstem.org/) from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "[Tokenizing](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is used to separate individual words in a string of text. For example, `hello world` is a string that can be broken into 2 tokens `hello` and `world` with a space delimiter. We'll use the [tokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize) from NLTK.\n",
    "\n",
    "We'll follow the guide and create 2 functions to tokenize and stem the text reviews.  \n",
    "\n",
    "- tokenize_and_stem: split the scripts into a list of words and stem each word\n",
    "- tokenize_only: split the scripts into a list of words\n",
    "\n",
    "These functions can be used to create a dictionary that will allow us to use stems for an algorithm but later convert the stems back to their full words for presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a tokenizer and stemmer that returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as its own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as its own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can use the stemming/tokenizing and tokenizing only functions to iterate over the list of scripts to create 2 vocabularies: one stemmed and one only tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Homer Simpson:  (HORRIFIED) Oh my God... space aliens!', \"Don't eat me!\", 'I have a wife and kids!', 'Eat them!']\n",
      "Homer Simpson:  (HORRIFIED) Oh my God... space aliens!\n",
      "homer\n",
      "simpson\n",
      ":\n",
      "(\n",
      "horrified\n",
      ")\n",
      "oh\n",
      "my\n",
      "god\n",
      "...\n",
      "space\n",
      "aliens\n",
      "!\n",
      "Don't eat me!\n",
      "do\n",
      "n't\n",
      "eat\n",
      "me\n",
      "!\n",
      "I have a wife and kids!\n",
      "i\n",
      "have\n",
      "a\n",
      "wife\n",
      "and\n",
      "kids\n",
      "!\n",
      "Eat them!\n",
      "eat\n",
      "them\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# need to download the necessary files (pickle model)\n",
    "#nltk.download('punkt')\n",
    "testLine = textList[3]\n",
    "out = nltk.sent_tokenize(testLine)\n",
    "print(out)\n",
    "for sent in nltk.sent_tokenize(testLine):\n",
    "    print(sent)\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not run this - it will take very long\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "#for i in textList:\n",
    "for i in textList[1]:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses,' tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 4.4004 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#x = 1000\n",
    "\n",
    "#testnames = nameList[:x]\n",
    "#testtexts = textList[:x]\n",
    "testtexts = textList\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in testtexts:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses,' tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "#print('Time elapsed: ' + str(round(end - start, 4)) + ' seconds for ' + str(x) + ' businesses.')\n",
    "print('Time elapsed: ' + str(round(end - start, 4)) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 88134 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dole</th>\n",
       "      <td>dole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext</th>\n",
       "      <td>ext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dole</th>\n",
       "      <td>dole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words\n",
       "dole          dole\n",
       "campaign  campaign\n",
       "stop          stop\n",
       "ext            ext\n",
       "dole          dole"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 20.7 ms, total: 3.07 s\n",
      "Wall time: 3.07 s\n",
      "(6690, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(testtexts) #fit the vectorizer to reviews\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 12 ms, total: 244 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starList = ysql['stars'].tolist()\n",
    "#nrevList = ysql['review_count'].tolist()\n",
    "\n",
    "#teststars = starList[:x]\n",
    "#testnrevs = nrevList[:x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reviews = { 'name': testnames, 'stars': teststars, 'review': testtexts, 'cluster': clusters, 'nreviews': testnrevs }\n",
    "\n",
    "#frame = pd.DataFrame(reviews, index = [clusters] , columns = ['stars', 'name', 'cluster', 'nreviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#frame['cluster'].value_counts() # number of businesses per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouped = frame['stars'].groupby(frame['cluster']) # group by cluster for aggregation purposes\n",
    "\n",
    "#grouped.mean() # average rank (1 to 5) per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
